import torchimport torch.nn.functional as Ffrom constants import *from utils import *fix_esm_path()import esmimport seaborn as snsimport torchimport torch.nn as nnimport torch.nn.functional as Ffrom rosetta_former.energy_vqvae import *from dataset import *import warningswarnings.filterwarnings("ignore", category=FutureWarning)batch_size = 64num_epochs = 5000device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")hbonds_all_residues = [14, 16, 18, 42, 44, 46, 61, 64, 68, 69, 72, 108, 110, 112, 119, 123, 145, 150, 163, 165, 167, 181, 185, 201, 220, 224]    nohbonds_all_residues = [42, 44, 61, 62, 69, 92, 94, 96, 112, 121, 145, 148, 150, 163, 165, 167, 181, 183, 185, 203, 205, 220, 222, 224]designable = set(nohbonds_all_residues + hbonds_all_residues)designable = sorted([x for x in designable])class Metric:  def __init__(self):    self.lst = 0.    self.sum = 0.    self.cnt = 0    self.avg = 0.  def update(self, val, cnt=1):    self.lst = val    self.sum += val * cnt    self.cnt += cnt    self.avg = self.sum / self.cntdef accuracy(pred, target):  acc = (pred.argmax(dim=1) == target)  return acc.to(pred).mean()class embedding_mlp(nn.Module):    def __init__(self, in_features, out_features):        super().__init__()        hidden_features = in_features * 4                self.norm1 = nn.LayerNorm(in_features)       # self.norm2 = nn.LayerNorm(in_features)        #self.mhsa = nn.MultiheadAttention(embed_dim=in_features, num_heads=24)                self.fc1 = nn.Linear(in_features, hidden_features)            self.act1 = nn.GELU()            self.fc_final = nn.Linear(hidden_features, out_features)                        self.init_weights()    def init_weights(self):        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in', nonlinearity='relu')        nn.init.kaiming_normal_(self.fc_final.weight, mode='fan_in', nonlinearity='relu')        nn.init.zeros_(self.fc1.bias)        nn.init.zeros_(self.fc_final.bias)    def forward(self, x):                B,S,D = x.size()        # norm_x = self.norm1(x)        # attn_x, _ = self.mhsa(norm_x,norm_x,norm_x)        # x = x + attn_x            # x = x[:,designable,:].view((B, -1))            x = x.mean(axis=1)                x = self.norm1(x)        x = self.fc1(x)        x = self.act1(x)        x = self.fc_final(x)                            return xdef train(model, optimizer, criterion, train_data_loader):    loss_metric = Metric()    acc_metric = Metric()        # Training loop    for epoch in range(num_epochs):        ctr = 0        #        save_torch_model(model_name, model, optimizer)                for idx, item in enumerate(train_data_loader):                        x,y = item            ctr += 1            optimizer.zero_grad()            # Forward pass            y_pred =  model(x)                        y = y.to(y_pred)            loss = criterion(y_pred, y.argmax(dim=1))                        acc = accuracy(y_pred, y.argmax(dim=1))            loss.backward()            optimizer.step()                        loss_metric.update(loss.item(), x.size(0))            acc_metric.update(acc.item(), x.size(0))                    print(' Train', f'Epoch: {epoch:03d} / {num_epochs:03d}',            f'Loss: {loss_metric.avg:7.4g}',            f'Accuracy: {acc_metric.avg:.3f}',            sep='   ')                      # Save model    if epoch % 200 == 0:        pass        #ToDo: save  #  save_torch_model(model_name, model, optimizer)    print("Training completed!")    sns.heatmap(y_pred.detach().numpy())base_dataset_path = "/Users/itayta/Desktop/prot_stuff/fitness_lndscp/fitness_learning/data/datasets/random_100k_train"dataset = EsmGfpDataset(sequences_path="%s/sequences.csv" % base_dataset_path,                        embeddings_path="%s/embeddings" % base_dataset_path,                        embedding_layer = -3,                        tokens_path="%s/tokens" % base_dataset_path,                        mode="embeddings")dataset.to(device)x,y = dataset[0]model = embedding_mlp(x.shape[1], y.shape[0])optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)train_data_loader = torch.utils.data.DataLoader()criterion = nn.CrossEntropyLoss()train(model, optimizer, criterion, train_data_loader)