#!/usr/bin/env python3# -*- coding: utf-8 -*-"""Created on Tue Dec 31 15:43:18 2024@author: itayta"""#!/usr/bin/env python3# -*- coding: utf-8 -*-"""Created on Fri Nov  1 23:43:09 2024@author: itayta"""#import huggingface_hubimport sys, osimport esmimport randomimport torchimport torch.nn.functional as Fimport loralib as loraimport numpy as npimport matplotlib.pyplot as pltimport subprocessimport argparsefrom constants import *from utils import *fix_esm_path()from esm.models.esm3 import ESM3from esm.sdk.api import ESM3InferenceClient, ESMProtein, GenerationConfigfrom esm.tokenization import get_model_tokenizersfrom esm.utils.constants.models import (    ESM3_FUNCTION_DECODER_V0,    ESM3_OPEN_SMALL,    ESM3_STRUCTURE_DECODER_V0,    ESM3_STRUCTURE_ENCODER_V0,)from esm.pretrained import (    ESM3_function_decoder_v0,    ESM3_sm_open_v0,    ESM3_structure_decoder_v0,    ESM3_structure_encoder_v0,)#from esm.tokenization import get_esm3_model_tokenizersfrom esm.tokenization.function_tokenizer import (    InterProQuantizedTokenizer as EsmFunctionTokenizer,)from esm.tokenization.sequence_tokenizer import (    EsmSequenceTokenizer,)from esm.utils.structure.protein_chain import ProteinChainfrom esm.utils.types import FunctionAnnotationfrom Bio import pairwise2from Bio.Seq import Seqfrom Bio.Align import substitution_matricesblosum62 = substitution_matrices.load("BLOSUM62")SEQUENCE_VOCAB = [    "<cls>", "<pad>", "<eos>", "<unk>",    "L", "A", "G", "V", "S", "E", "R", "T", "I", "D", "P", "K",    "Q", "N", "F", "Y", "M", "H", "W", "C", "X", "B", "U", "Z",    "O", ".", "-", "|",    "<mask>",]def find_idx_for_aa(token):    return(np.where(np.array(SEQUENCE_VOCAB) == token)[0][0])@torch.no_grad()def calculate_esm_fitness_score_from_prob(pos_prob_matrix, wt_token, variant_token_tensor):        wt_one_hot = torch.nn.functional.one_hot(torch.tensor(wt_token), pos_prob_matrix.shape[1])    variant_one_hot = torch.nn.functional.one_hot(torch.tensor(variant_token_tensor), pos_prob_matrix.shape[1])       # return (torch.mean(torch.log(torch.sum(pos_prob_matrix * variant_one_hot, dim=1)) - torch.log(torch.sum(pos_prob_matrix * wt_one_hot, dim=1))))    return torch.mean((torch.log(torch.einsum("BAC,AC->BA", variant_one_hot[:,:,:].to(torch.float), pos_prob_matrix)) - torch.log(torch.sum(pos_prob_matrix * wt_one_hot, dim=1))), dim=1)    #huggingface_hub.login(token=HF_TOKEN)def assign_lora_weights(model,                         save_lora_weights=False,                        print_parameters=True,                        train_bias=False):        if LORA_TRANSFORMER_LINEAR_WEIGHTS:                for transformer_block_idx in range(0, N_LAYERS):                        model.transformer.blocks[transformer_block_idx].attn.layernorm_qkv[1] =\                lora.Linear(model.transformer.blocks[transformer_block_idx].attn.layernorm_qkv[1].in_features,                             model.transformer.blocks[transformer_block_idx].attn.layernorm_qkv[1].out_features,                             LORA_R)                model.transformer.blocks[transformer_block_idx].attn.out_proj =\                lora.Linear(model.transformer.blocks[transformer_block_idx].attn.out_proj.in_features,                             model.transformer.blocks[transformer_block_idx].attn.out_proj.out_features,                             LORA_R)                model.transformer.blocks[transformer_block_idx].ffn[1] =\                lora.Linear(model.transformer.blocks[transformer_block_idx].ffn[1].in_features,                             model.transformer.blocks[transformer_block_idx].ffn[1].out_features,                             LORA_R)                model.transformer.blocks[transformer_block_idx].ffn[3] =\                lora.Linear(model.transformer.blocks[transformer_block_idx].ffn[3].in_features,                             model.transformer.blocks[transformer_block_idx].ffn[3].out_features,                             LORA_R)                                        if transformer_block_idx == 0:                model.transformer.blocks[transformer_block_idx].geom_attn.proj =\                    lora.Linear(model.transformer.blocks[transformer_block_idx].geom_attn.proj.in_features,                                 model.transformer.blocks[transformer_block_idx].geom_attn.proj.out_features,                                 LORA_R)                                    model.transformer.blocks[transformer_block_idx].geom_attn.out_proj =\                    lora.Linear(model.transformer.blocks[transformer_block_idx].geom_attn.out_proj.in_features,                                 model.transformer.blocks[transformer_block_idx].geom_attn.out_proj.out_features,                                 LORA_R)        if train_bias:        bias_str = "all"    else:        bias_str = "none"            lora.mark_only_lora_as_trainable(model,                                     bias=bias_str)        if print_parameters:                params  = [p.numel() for p in model.parameters()]        trainable_params = [p.numel() for p in model.parameters() if p.requires_grad==True]                        print("Training %d of %d params (%.3f), training bias: %s" %\              (sum(trainable_params), sum(params), sum(trainable_params) / sum(params), "True" if train_bias else "False"))        if save_lora_weights:        torch.save(lora.lora_state_dict(model, bias=bias_str),                    "%s/%s" % (WEIGHTS_PATH, LORA_WEIGHTS_FIlE_NAME))            return(model)def load_model(lora_weights=True,                device="cpu",               weights_path=WEIGHTS_PATH,               weights_file=MODEL_WEIGHTS_FILE_NAME,               lora_weights_file=LORA_WEIGHTS_FIlE_NAME):    with torch.device(device):        model = ESM3(            d_model=D_MODEL,            n_heads=N_HEADS,            v_heads=V_HEADS,            n_layers=N_LAYERS,            structure_encoder_fn=ESM3_structure_encoder_v0,            structure_decoder_fn=ESM3_structure_decoder_v0,            function_decoder_fn=ESM3_function_decoder_v0,            tokenizers=get_model_tokenizers(ESM3_OPEN_SMALL),        ).eval()                    if lora_weights:        model = assign_lora_weights(model)        state_dict = torch.load("%s/%s" % (weights_path, weights_file), map_location=device)    model.load_state_dict(state_dict, strict=False)        if lora_weights:        lora_state_dict = torch.load("%s/%s" % (weights_path, lora_weights_file), map_location=device)        model.load_state_dict(lora_state_dict, strict=False)        return (model)@torch.enable_grad()def calculate_esm_fitness():    tokenizer = EsmSequenceTokenizer()    tokenizers = get_model_tokenizers()    encoder = ESM3_structure_encoder_v0("cpu")    decoder = ESM3_structure_decoder_v0("cpu")    model =  load_model()    #chain = ProteinChain.from_rcsb("1utn", "A")    # Read PDB    chain = ProteinChain.from_pdb(WT_PDB_PATH)    base_path = "/Users/itayta/Desktop/prot_stuff/fitness_lndscp/fitness_learning/data/configuration/"    pt2 = "%s/fixed_unique_gfp_sequence_dataset.csv" % base_path    pt = "%s/gfp_physical.csv" % base_path     dsdf = pd.read_csv(pt2)        #sequences_df = pd.read_csv(pt)            # pos = [int(x[1:]) for x in dsdf.columns[40:62].to_list()]    # wt_aa_in_pos = [int(x[0]) for x in dsdf.columns[40:62].to_list()]        # aligned = {"mutated_positions":[],    #            "aligned_sequences":[],    #            "nmuts":[]}        # for sequences_idx in range(sequences_df.shape[0]):    #     tp = sequences_df["Protein Sequence"][sequences_idx]    #     nmuts = sequences_df["# Mutations"][sequences_idx]    #     pdb_sequence = chain.sequence            #     chain_seq = Seq(pdb_sequence)    #     ref_seq = Seq(tp)            #     alm = pairwise2.align.localds(ref_seq, chain_seq, blosum62, -20, -1)    #     aligned_ref_seq = alm[0].seqA[alm[0].start:alm[0].end]    #     aligned_wt_seq = alm[0].seqB[alm[0].start:alm[0].end]                #     misaligned_positions = np.where([x == "-" for x in aligned_wt_seq])    #     mutated_positions = np.where([ord(aligned_ref_seq[i]) != ord(aligned_wt_seq[i]) for i in range(len(aligned_ref_seq))])[0]            #     mutated_positions = mutated_positions[~np.isin(mutated_positions, misaligned_positions)]            #     aligned["mutated_positions"].append(mutated_positions)    #     aligned["nmuts"].append((nmuts, len(mutated_positions)))    #     aligned["aligned_sequences"].append((aligned_ref_seq, aligned_wt_seq))        # Structure tokens    coords, plddt, residue_index = chain.to_structure_encoder_inputs()    coords = coords.cpu()    plddt = plddt.cpu()    residue_index = residue_index.cpu()    _, structure_tokens = encoder.encode(coords, residue_index=residue_index)        # Add BOS/EOS padding    coords = F.pad(coords, (0, 0, 0, 0, 1, 1), value=torch.inf)    plddt = F.pad(plddt, (1, 1), value=0)    structure_tokens = F.pad(structure_tokens, (1, 1), value=0)    structure_tokens[:, 0] = 4098    structure_tokens[:, -1] = 4097   #structure_tokens[:,pos] = 4096 # Because padd was already added, so pos 1 in array is indexing aa number 1 ()    m1_pos = [p-1 for p in pos]    # Sequence tokens    masked_sequence = "<mask>".join("".join(["_" if i in m1_pos else pdb_sequence[i] for i in range(len(pdb_sequence))]).split("_"))    sequence = tokenizers.sequence.encode(masked_sequence)        #sequence = tokenizer.encode('IVGGEDAGAHTRPYQVALDRGRHICGGSLINERWVVTAAHCYRDGWTAHAGEHNIRVDEGTEQRIPASKQWVHPNYDPSTLDNDIMLVKLATPATLDENVAPIPLPTVPPVEGTVCTVSGWGNTKEEGSSYPTTLQKLEVPILSDEVCQAAYPGRITPNMFCAGYLEGGKDTCQGDSGGPFVCNGELHGIVSWGEGCAQPNKPGVYTRVYLYIGWIEETIATN')    output = model.forward(structure_coords=coords,                            per_res_plddt=plddt,                            structure_tokens=structure_tokens,                           sequence_tokens=torch.tensor(sequence, dtype=torch.int64).reshape((1,-1))    )                sequence_tokens = torch.argmax(output.sequence_logits, dim=-1)    function_tokens = torch.argmax(output.function_logits, dim=-1)    sasa_tokens = torch.argmax(output.sasa_logits, dim=-1)    ss8_tokens = torch.argmax(output.secondary_structure_logits, dim=-1)    #residue_tokens = torch.argmax(output.residue_logits, dim=-1)        sequence_delta_before_enrich = sequence_tokens.view((-1)) - torch.tensor(sequence, dtype=torch.int64).reshape((-1))    plt.hist(sequence_delta_before_enrich)            output2 = model.forward(structure_coords=coords,        per_res_plddt=plddt, structure_tokens=structure_tokens,        sequence_tokens=torch.tensor(sequence, dtype=torch.int64).reshape((1,-1)),        function_tokens=function_tokens,        sasa_tokens=sasa_tokens,        #residue_annotation_tokens=residue_tokens,        ss8_tokens=ss8_tokens)            prob = output2.sequence_logits.squeeze(dim=1).softmax(dim=1)    mut_positions = prob[pos,:]     mut_positions = mut_positions[:,0:33]        N_SEQ = dsdf.shape[0]    BATCH_SIZE = 5000    N_BATCHES = N_SEQ // BATCH_SIZE        wt_token = tokenizers.sequence.encode("".join(aa_in_pos))[1:-1]    pos_prob_matrix = mut_positions        esm_fitness_scores = torch.tensor([])        for i in range(N_BATCHES):        s_i = i * BATCH_SIZE                if i == (N_BATCHES - 1):            e_i = N_SEQ        else:            e_i = (i + 1) * BATCH_SIZE                            batch_variant_token_tensor = torch.tensor(tokenizers.sequence.encode("".join(dsdf["sequence"][s_i:e_i].to_list()))[1:-1]).reshape((-1, 22))        batch_fitness = calculate_esm_fitness_score_from_prob(pos_prob_matrix, wt_token, batch_variant_token_tensor)        esm_fitness_scores = torch.cat([esm_fitness_scores, batch_fitness], dim=0)                if i % 30 == 0:            print("Running %d sequences fitness (%d to %d)" % (BATCH_SIZE, s_i, e_i))    esm_fitness_df = pd.DataFrame(esm_fitness_scores.detach().numpy())        pt3 = "%s/esm_fitness_df.csv" % base_path         esm_fitness_df.to_csv(pt3)#def master():    calculate_esm_fitness() 