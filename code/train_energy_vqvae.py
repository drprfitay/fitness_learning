import torchimport torch.nn.functional as Ffrom constants import *from utils import *fix_esm_path()import esmimport torchfrom rosetta_former.energy_vqvae import *from dataset import *# HyperparametersENERGY_INPUT_DIM = 20ENERGY_D_OUT = 512ENERGY_D_MODEL = 1024ENERGY_N_CODEBOOK = 32# 8192ENCODER_DEPTH=4DECODER_DEPTH=16commitment_cost = .25batch_size = 32num_epochs = 10dataset_name = "gfp_train_1"def save_torch_model(model_name, model, optimizer):    file_path = "%s/%s.pth" % (WEIGHTS_PATH, model_name)    torch.save({        'model_state_dict': model.state_dict(),        'optimizer_state_dict': optimizer.state_dict()    }, file_path)    def load_torch_model(model_name, model, optimizer=None):    file_path = "%s/%s.pth" % (WEIGHTS_PATH, model_name)    checkpoint = torch.load(file_path)    model.load_state_dict(checkpoint['model_state_dict'])    if optimizer is not None:        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])            return(model)        def train(model_name, model, optimizer, train_data_loader, entropy_lambda=-.5):        dataset = RawTokensDataset(dataset_name, mode="energy")    # Training loop    for epoch in range(num_epochs):        ctr = 0                save_torch_model(model_name, model, optimizer)                for batch in train_data_loader:                        ctr += 1            optimizer.zero_grad()            # Forward pass            x_recon, vq_loss, encoding_ind = model(batch)            counts = torch.unique(encoding_ind, return_counts=True)[1]            probs = counts / sum(counts)            # Reconstruction loss            recon_loss = F.mse_loss(x_recon, batch)                        entropy_loss = torch.sum(-torch.log(probs) * probs)            # Total loss            loss = recon_loss + vq_loss + entropy_lambda * entropy_loss                        print("\t(E:%d) %d loss: %.3f [recon %.3f, vqvae %.3f, entropy %.3f]" % (epoch + 1, ctr + 1, loss.item(), recon_loss.item(), vq_loss.item(), entropy_loss.item()))            loss.backward()            optimizer.step()        print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")    save_torch_model(model_name, model, optimizer)    print("Training completed!")# dataset = RawTokensDataset(dataset_name, mode="energy")# train_data_loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)# Initialize model, optimizer, and loss function# model = EnergyVQVAE(ENERGY_INPUT_DIM, #                     ENERGY_D_MODEL, #                     ENERGY_D_OUT, #                     ENERGY_N_CODEBOOK, #                     commitment_cost,#                     ENCODER_DEPTH,#                     DECODER_DEPTH)# optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)#model = load_torch_model("energy_vqvae_v1", model)#train("energy_vqvae_v0_deeper", model, optimizer, train_data_loader)# model = load_torch_model("energy_vqvae_v1", model)# x = dataset[5]# a,b=model(x.unsqueeze(dim=0))# rec = a.squeeze()# [scipy.stats.pearsonr(x[i,:].detach().numpy(), rec[i,:].detach().numpy())[0] for i in range(225)]#class EnergyLogisticRegression(nn.Module):    def __init__(self, in_dim, dtype=torch.float):        super().__init__()        self.regression_weights = nn.Linear(in_dim, 1, dtype=dtype)        nn.init.xavier_uniform_(self.regression_weights.weight)        def forward(self, x):        z = self.regression_weights(x)        return(z)        class EnergyMLP(nn.Module):    def __init__(self, input_dim):        super().__init__()        self.layer1 = nn.Linear(input_dim, 1024)  # First hidden layer with 512 neurons        self.layer2 = nn.Linear(1024,  512)  # Second hidden layer with 128 neurons        #self.layer3 = nn.Linear(512, 512)  # Second hidden layer with 128 neurons        # self.layer4 = nn.Linear(512, 128)  # Second hidden layer with 128 neurons        # self.layer5 = nn.Linear(128, 128)  # Second hidden layer with 128 neurons        self.output_layer = nn.Linear(512, 1)  # Output layer (binary classification)                self.relu = nn.ReLU()  # Non-linear activation function                nn.init.xavier_uniform_(self.layer1.weight)        nn.init.xavier_uniform_(self.layer2.weight)        #nn.init.xavier_uniform_(self.layer3.weight)        # nn.init.xavier_uniform_(self.layer4.weight)        # nn.init.xavier_uniform_(self.layer5.weight)        nn.init.xavier_uniform_(self.output_layer.weight)    def forward(self, x):        x = self.relu(self.layer1(x))  # Pass through first hidden layer + ReLU        x = self.relu(self.layer2(x))  # Pass through second hidden layer + ReLU        #x = self.relu(self.layer3(x))  # Pass through second hidden layer + ReLU        #x = self.relu(self.layer4(x))  # Pass through second hidden layer + ReLU        #x = self.relu(self.layer5(x))  # Pass through second hidden layer + ReLU        x = self.output_layer(x)          return x    def train_elr(model_name, model, optimizer, criterion, train_data_loader, num_epochs=10000):            # Training loop    for epoch in range(num_epochs):        ctr = 0                save_torch_model(model_name, model, optimizer)                for x, y_true in train_data_loader:                        ctr += 1            optimizer.zero_grad()            # Forward pass            #logits = model(x.view((x.shape[0],-1)).to(torch.float))            logits = model(x.to(torch.float))            loss = criterion(logits.squeeze(dim=1), y_true)                    loss.backward()            optimizer.step()            y_pred = torch.sigmoid(logits)                        acc = ((y_pred > .5).squeeze(dim=1) == y_true).float().mean().item()                        # if ctr % 5 == 0:                            #     #print(len(torch.unique((y_pred > 0))))            #     print("\t(E:%d) %d loss: %.3f, acc: %.3f" % (epoch + 1, ctr + 1, loss.item(), acc))        print(f"Epoch {epoch+1}, Loss: {loss.item():.4f} Acc {acc:.4f}")    save_torch_model(model_name, model, optimizer)    print("Training completed!")            #dataset = RawTokensDataset(dataset_name, mode="energy", return_labels=True)dataset = EnergyDataset(dataset_name, size=1000)train_data_loader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=True)#S,H = dataset[0][0].size()#model = EnergyLogisticRegression(S*H)#model = EnergyMLP(S*H)S = dataset[0][0].size()#model = EnergyLogisticRegression(S[0])model = EnergyMLP(S[0])optimizer = torch.optim.Adam(model.parameters(), lr=3e-5, weight_decay=0.1)criterion = nn.BCEWithLogitsLoss()#train_elr("energy_lr_v0", model, optimizer, criterion, train_data_loader)