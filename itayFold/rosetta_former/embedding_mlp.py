# -*- coding: utf-8 -*-import torchimport torch.nn as nnimport torch.nn.functional as Fclass EmbeddingMLP(nn.Module):    def __init__(self, in_features, out_features):        super().__init__()        hidden_features = in_features * 4                self.norm1 = nn.LayerNorm(in_features)       # self.norm2 = nn.LayerNorm(in_features)        #self.mhsa = nn.MultiheadAttention(embed_dim=in_features, num_heads=24)                self.fc1 = nn.Linear(in_features, hidden_features)            self.act1 = nn.GELU()            self.fc_final = nn.Linear(hidden_features, out_features)                        self.init_weights()    def init_weights(self):        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in', nonlinearity='relu')        nn.init.kaiming_normal_(self.fc_final.weight, mode='fan_in', nonlinearity='relu')        nn.init.zeros_(self.fc1.bias)        nn.init.zeros_(self.fc_final.bias)    def forward(self, x):                B,S,D = x.size()        # norm_x = self.norm1(x)        # attn_x, _ = self.mhsa(norm_x,norm_x,norm_x)        # x = x + attn_x            # x = x[:,designable,:].view((B, -1))            x = x.mean(axis=1)                x = self.norm1(x)        x = self.fc1(x)        x = self.act1(x)        x = self.fc_final(x)                            return x